#!/usr/bin/env python3

import os
import sys
import json
import urllib.request
import feedparser

from arxiv import log
from arxiv import get_params
from arxiv import get_state

from arxiv import wait
from arxiv import sanitize
from arxiv import get_arg_parser

from arxiv import escape_filename

from arxiv import download

def main ( kwargs ) :

    log('================================')
    params = get_params ( kwargs )
    _state = get_state(params)
    log('================================')

    return run_once( _state = _state , **params )

def run_once ( _state = None , state = None , throttle = None , batch = None ,
        storage = None , metadata = None , queries = None ,
        request = None , format = None, timeout = None, **kwargs ) :

    storage = os.path.expanduser(storage)
    metadata = os.path.expanduser(metadata)

    rc = 0

    for search_query in queries:
        log("search_query", search_query)
        start = _state.setdefault(search_query, 0)

        query_url = request.format(search_query=search_query, start=start, max_results=batch)
        log(query_url)

        wait(throttle)
        try:
            response = urllib.request.urlopen(query_url, timeout=timeout)
        except Exception as e:
            log('Failed to download', url)
            log(e)
            break

        document = feedparser.parse(response)
        feed = document.feed
        if int(feed.opensearch_startindex) + int(feed.opensearch_itemsperpage) < int(feed.opensearch_totalresults):
            rc = 127

        for entry in document.entries:

            uid = entry.id.split('/abs/')[-1]
            title = sanitize(entry.title)
            authors = entry.authors
            authors_names = list(map(lambda x: sanitize(x.name), authors))
            links = entry.links
            published = entry.published
            updated = entry.updated
            summary = sanitize(entry.summary)
            category = entry.category

            year = published[:4]
            month = published[5:7]
            day = published[8:10]

            log('url: {}'.format(entry.id))
            log('arxiv-id: {}'.format(uid))
            log('Published: {}'.format(published))
            log('Title:  {}'.format(title))
            log('Authors:  {}'.format(', '.join(authors_names)))

            _metadata = entry

            slug = escape_filename(uid)

            filename = "{}.pdf".format(slug)

            path = "{}/{}".format(storage, filename)

            try:
                exists = os.path.getsize(path) > 0
            except:
                exists = False

            if exists :
                # TODO add option to check if size in HTTP header corresponds
                log("{} already exists, skipping".format(path))
            else:
                url = next(filter(lambda x: x.type == 'application/pdf', links)).href
                os.makedirs(storage, exist_ok=True)

                wait(throttle)

                log("Downloading {} to {}.".format(url, path))
                try:
                    download(url, path, timeout=timeout)
                except Exception as e:
                    log("Failed to download", url)
                    log(e)
                    try:
                        log("Cleaning up...")
                        if os.path.exists(path):
                            log("Removing", path)
                            os.remove(path)
                            log("Success :)")
                        else:
                            log("Nothing to do!")
                    except Exception as e:
                        log("Failed :(")
                        log(e)
                    break

            _state[search_query] += 1

            metadata_path = '{}/{}.json'.format(metadata, slug)

            os.makedirs(metadata, exist_ok=True)
            with open(metadata_path, 'w') as fp:
                # https://docs.python.org/3/library/json.html?highlight=json#json.dump
                json.dump(_metadata, fp, separators=(',', ':'))

    os.makedirs(os.path.dirname(state), exist_ok=True)
    with open(state, 'w') as fp:
        json.dump(_state, fp, separators=(',', ':'))

    return rc

if __name__ == '__main__':
    parser = get_arg_parser()
    args = parser.parse_args()
    rc = main(vars(args))
    sys.exit(rc)
